{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-09-19T08:33:15.794752Z",
     "end_time": "2023-09-19T08:33:19.810341Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import os, pickle\n",
    "\n",
    "config = utils.create_config(features = ['distance.origin-prob','distance.head-prob', 'pose.prob_x','pose.prob_y'],\n",
    "                            contamination = 0.04,\n",
    "                            grouped_range = 60,\n",
    "                            add_golay = False,\n",
    "                            add_mv_stats = False,\n",
    "                            show_true_annotations = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bouts_dict = utils.read_pickle('bouts_dict.pkl')\n",
    "true_peak_annotations_df = utils.read_pickle('true_annotations.pkl')\n",
    "fly_db = utils.create_fly_database(bouts_dict, true_peak_annotations_df=true_peak_annotations_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T00:34:39.800847Z",
     "start_time": "2023-09-11T00:34:37.695105Z"
    }
   },
   "id": "d90df642c94f824a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bouts_dict = utils.read_pickle('bouts_dict.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T00:38:29.577801Z",
     "start_time": "2023-09-11T00:38:27.828329Z"
    }
   },
   "id": "c84433f3aa4e7e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "trace = bouts_dict['Fly08022022_6d_SD_B']['pose.thor_post_x'][0]\n",
    "plt.plot(bouts_dict['Fly08022022_6d_SD_B']['pose.thor_post_x'][0])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:23:42.249238Z",
     "start_time": "2023-09-11T01:23:42.115218Z"
    }
   },
   "id": "b7ea7c2480b955d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trace.std()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:23:43.417849Z",
     "start_time": "2023-09-11T01:23:43.413334Z"
    }
   },
   "id": "44242990d62cbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trace = bouts_dict['Fly08022022_6d_SD_B']['distance.origin-thor_post'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:27:56.817630Z",
     "start_time": "2023-09-11T01:27:56.806828Z"
    }
   },
   "id": "b2babe5e3ed74549"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(trace)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:27:57.767458Z",
     "start_time": "2023-09-11T01:27:57.636235Z"
    }
   },
   "id": "95c1f6962964646d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trace.std()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:27:58.567861Z",
     "start_time": "2023-09-11T01:27:58.563910Z"
    }
   },
   "id": "bb5d638a07025acc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bouts_dict['Fly08022022_6d_SD_B'].columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:40:31.548978Z",
     "start_time": "2023-09-11T01:40:31.537021Z"
    }
   },
   "id": "e190edf96a5fbf10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = ['distance.origin-thor_post','distance.origin-halt','distance.head-prob','distance.avg(origin-joint1,origin-joint2,origin-joint3)']\n",
    "\n",
    "# Dictionary to store standard deviations, organized by column\n",
    "std_dict = {col: [] for col in columns_of_interest}\n",
    "\n",
    "# Your existing dictionary of DataFrames\n",
    "dict_of_dfs = bouts_dict\n",
    "\n",
    "# Iterate over experiments (keys)\n",
    "for exp_key, df in dict_of_dfs.items():\n",
    "    \n",
    "    # Iterate only over specified columns\n",
    "    for col in columns_of_interest:\n",
    "        \n",
    "        # Skip if the column is not in the DataFrame\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over rows in the specific column\n",
    "        for idx, cell_data in enumerate(df[col]):\n",
    "            \n",
    "            # Convert cell_data to a NumPy array for easier calculations\n",
    "            time_series = np.array(cell_data)\n",
    "            \n",
    "            # Compute standard deviation\n",
    "            std_dev = np.std(time_series)\n",
    "            \n",
    "            # Store this standard deviation in the dictionary under the appropriate column\n",
    "            std_dict[col].append(std_dev)\n",
    "\n",
    "# Create histograms for each column\n",
    "for col, std_list in std_dict.items():\n",
    "    \n",
    "    # Convert std_list to a NumPy array for easier manipulation\n",
    "    std_array = np.array(std_list)\n",
    "    \n",
    "    plt.figure()  # Create a new figure for each column\n",
    "    plt.hist(std_array, bins=50)  # You can change the number of bins as needed\n",
    "    plt.xlabel('Standard Deviation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Standard Deviations for Column {col}')\n",
    "    plt.show()\n",
    "\n",
    "with open('bouts_std.pkl', 'wb') as f:\n",
    "    pickle.dump(std_dict, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:49:58.973437Z",
     "start_time": "2023-09-11T01:49:57.714279Z"
    }
   },
   "id": "69242046a17a6f76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_std_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T01:43:06.949780Z",
     "start_time": "2023-09-11T01:43:06.919623Z"
    }
   },
   "id": "d36e6a684d86bc71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# List of columns of interest\n",
    "columns_of_interest = ['distance.origin-thor_post', 'distance.origin-halt', 'distance.head-prob', 'distance.avg(origin-joint1,origin-joint2,origin-joint3)']\n",
    "\n",
    "# Dictionary to store standard deviations, organized by column\n",
    "std_dict = {col: [] for col in columns_of_interest}\n",
    "\n",
    "# Additional dictionaries to store experiment keys and indices corresponding to each standard deviation\n",
    "exp_keys_dict = {col: [] for col in columns_of_interest}\n",
    "idxs_dict = {col: [] for col in columns_of_interest}\n",
    "\n",
    "# Dictionaries to store filtered and removed standard deviations\n",
    "filtered_bouts_dict = {col: [] for col in columns_of_interest}\n",
    "removed_bouts_dict = {col: [] for col in columns_of_interest}\n",
    "\n",
    "# Your existing dictionary of DataFrames (for demonstration, replace with your actual dictionary)\n",
    "dict_of_dfs = bouts_dict  # Replace this with your bouts_dict\n",
    "\n",
    "# Iterate over experiments (keys)\n",
    "for exp_key, df in dict_of_dfs.items():\n",
    "    \n",
    "    # Iterate only over specified columns\n",
    "    for col in columns_of_interest:\n",
    "        \n",
    "        # Skip if the column is not in the DataFrame\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over rows in the specific column\n",
    "        for idx, cell_data in enumerate(df[col]):\n",
    "            \n",
    "            # Convert cell_data to a NumPy array for easier calculations\n",
    "            time_series = np.array(cell_data)\n",
    "            \n",
    "            # Compute standard deviation\n",
    "            std_dev = np.std(time_series)\n",
    "            \n",
    "            # Store this standard deviation in the dictionary under the appropriate column\n",
    "            std_dict[col].append(std_dev)\n",
    "            exp_keys_dict[col].append(exp_key)\n",
    "            idxs_dict[col].append(idx)\n",
    "\n",
    "# Calculate the 95th percentile for the 'distance.origin-thor_post' column\n",
    "percentile_95 = np.percentile(std_dict['distance.origin-thor_post'], 95)\n",
    "\n",
    "# Separate data based on the 95th percentile\n",
    "for col in columns_of_interest:\n",
    "    for exp_key, idx, std_dev in zip(exp_keys_dict[col], idxs_dict[col], std_dict[col]):\n",
    "        if col == 'distance.origin-thor_post' and std_dev > percentile_95:\n",
    "            removed_bouts_dict[col].append((exp_key, idx, std_dev))\n",
    "        else:\n",
    "            filtered_bouts_dict[col].append((exp_key, idx, std_dev))\n",
    "\n",
    "# Create histograms for each column in filtered_bouts_dict\n",
    "for col, std_list in filtered_bouts_dict.items():\n",
    "    \n",
    "    # Extract the standard deviation values from the tuples\n",
    "    std_array = np.array([t[2] for t in std_list])\n",
    "    \n",
    "    plt.figure()  # Create a new figure for each column\n",
    "    plt.hist(std_array, bins=50)  # You can change the number of bins as needed\n",
    "    plt.xlabel('Standard Deviation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Standard Deviations for Column {col} (Filtered)')\n",
    "    plt.show()\n",
    "\n",
    "# Save the filtered and removed dictionaries\n",
    "with open('filtered_bouts_std.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_bouts_dict, f)\n",
    "\n",
    "with open('removed_bouts_std.pkl', 'wb') as f:\n",
    "    pickle.dump(removed_bouts_dict, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T02:21:57.990991Z",
     "start_time": "2023-09-11T02:21:55.613669Z"
    }
   },
   "id": "c8b0b86af72d3ef3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Step 1: Calculate the 95th percentile based on the standard deviations of \"distance.origin-thor_post\"\n",
    "all_std_values = []\n",
    "for exp_key, df in bouts_dict.items():\n",
    "    if \"distance.origin-thor_post\" in df.columns:\n",
    "        for time_series in df[\"distance.origin-thor_post\"]:\n",
    "            all_std_values.append(np.std(time_series) / np.sqrt(len(time_series)))\n",
    "\n",
    "# Calculate the 95th percentile\n",
    "percentile_95 = np.percentile(all_std_values, 95)\n",
    "\n",
    "# Step 2: Create two new dictionaries of DataFrames\n",
    "filtered_bouts_dict = {}\n",
    "removed_bouts_dict = {}\n",
    "\n",
    "for exp_key, df in bouts_dict.items():\n",
    "    if \"distance.origin-thor_post\" not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty DataFrames with the same columns for each experiment\n",
    "    filtered_df = pd.DataFrame(columns=df.columns)\n",
    "    removed_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        time_series = row[\"distance.origin-thor_post\"]\n",
    "        std_dev = np.std(time_series) / np.sqrt(len(time_series))\n",
    "        \n",
    "        if std_dev > percentile_95:\n",
    "            removed_df = removed_df.append(row)\n",
    "        else:\n",
    "            filtered_df = filtered_df.append(row)\n",
    "    \n",
    "    # Store the filtered and removed DataFrames in their respective dictionaries\n",
    "    filtered_bouts_dict[exp_key] = filtered_df.reset_index(drop=True)\n",
    "    removed_bouts_dict[exp_key] = removed_df.reset_index(drop=True)\n",
    "\n",
    "filtered_bouts_dict, removed_bouts_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T23:31:41.220691Z",
     "start_time": "2023-09-12T23:31:06.926080Z"
    }
   },
   "id": "3fd212cc85a48ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "def create_wavelet_filter_bank(signal_length, sampling_frequency, freq_limits, wavelet_name):\n",
    "    # Create wavelet\n",
    "    wavelet = pywt.ContinuousWavelet(wavelet_name)\n",
    "    \n",
    "    # Create scales\n",
    "    scales = np.arange(1, signal_length, 1)\n",
    "    \n",
    "    # Create frequencies\n",
    "    frequencies = pywt.scale2frequency(wavelet, scales) * sampling_frequency\n",
    "    \n",
    "    # Filter frequencies within the limits\n",
    "    valid_indices = np.where((frequencies >= freq_limits[0]) & (frequencies <= freq_limits[1]))\n",
    "    valid_frequencies = frequencies[valid_indices]\n",
    "    valid_scales = scales[valid_indices]\n",
    "    \n",
    "    return valid_scales, valid_frequencies\n",
    "\n",
    "def get_scale_spectrum(filter_bank, signal):\n",
    "    valid_scales, _ = filter_bank\n",
    "    coeffs, _ = pywt.cwt(signal, valid_scales, 'cmor')\n",
    "    return coeffs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T22:29:10.169715Z",
     "start_time": "2023-09-12T22:29:10.162347Z"
    }
   },
   "id": "f658f7437b967b6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signal_length = 1000  # Replace with size(tsSnap{i}, 1)\n",
    "sampling_frequency = 30\n",
    "freq_limits = [0.25, 1]\n",
    "wavelet_name = 'cmor'  # or any other wavelet type you want to use\n",
    "\n",
    "# Create the filter bank\n",
    "filter_bank = create_wavelet_filter_bank(signal_length, sampling_frequency, freq_limits, wavelet_name)\n",
    "\n",
    "\n",
    "# Get the scale spectrum\n",
    "coeffs = get_scale_spectrum(filter_bank, trace)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T22:29:13.053784Z",
     "start_time": "2023-09-12T22:29:13.027306Z"
    }
   },
   "id": "2d7b52375cf55daf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(coeffs, aspect='auto', cmap='jet', interpolation='nearest')\n",
    "plt.colorbar(label='Coefficient Value')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Scale')\n",
    "plt.title('Wavelet Transform Coefficients')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T22:45:34.725052Z",
     "start_time": "2023-09-12T22:45:34.509602Z"
    }
   },
   "id": "51af884046f4ecc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save some portion of it:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T07:02:23.637322Z",
     "start_time": "2023-09-13T07:02:23.518605Z"
    }
   },
   "id": "9b51dd0ba1e8d3a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "dict_of_dfs = bouts_dict\n",
    "\n",
    "\n",
    "\n",
    "# Slice the first 5 key-value pairs from the dictionary\n",
    "sliced_dict_of_dfs = {k: dict_of_dfs[k] for k in list(dict_of_dfs.keys())[:5]}\n",
    "\n",
    "# Save the sliced dictionary in .pkl format\n",
    "with open('sliced_dict_of_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(sliced_dict_of_dfs, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-19T14:23:12.646363Z",
     "end_time": "2023-09-19T14:23:12.678486Z"
    }
   },
   "id": "6c3440c6ce72f303"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#one approach to achieve this, requires further tuning though\n",
    "\n",
    "# Importing necessary libraries and modules\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for smoothing the time series using Savitzky-Golay filter\n",
    "def smooth_time_series(time_series, window_length=11, polyorder=3):\n",
    "    return savgol_filter(time_series, window_length, polyorder)\n",
    "\n",
    "# Function to check if the time series is stationary\n",
    "def is_stationary(time_series, threshold=5):\n",
    "    return np.max(time_series) - np.min(time_series) <= threshold\n",
    "\n",
    "# Function to segment the time series\n",
    "def segment_time_series(time_series, window_size=75):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start + window_size <= len(time_series):\n",
    "        segments.append(time_series[start:start + window_size])\n",
    "        start += window_size // 2  # 50% overlap\n",
    "    return segments\n",
    "\n",
    "# Function to calculate correlation between two segments\n",
    "def calculate_correlation(segment1, segment2):\n",
    "    return np.corrcoef(segment1, segment2)[0, 1]\n",
    "\n",
    "# Function to check for significant change in the segment\n",
    "def significant_change(segment, rate_threshold=15, points_threshold=30):\n",
    "    return np.max(segment) - np.min(segment) >= rate_threshold and len(segment) >= points_threshold\n",
    "\n",
    "# Initialize the annotation dictionary\n",
    "annotations = {}\n",
    "\n",
    "# Iterate through each experiment and DataFrame\n",
    "for exp_key, df in bouts_dict.items():\n",
    "    annotation = []\n",
    "    \n",
    "    # If \"distance.origin-thor_post\" not in columns, continue to next iteration\n",
    "    if \"distance.origin-thor_post\" not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    # Apply Savitzky-Golay smoothing to the time series data\n",
    "    df_smooth = df.apply(lambda x: smooth_time_series(x) if isinstance(x, np.ndarray) else x)\n",
    "    \n",
    "    # Check if the \"distance.origin-thor_post\" column is stationary\n",
    "    if is_stationary(df_smooth['distance.origin-thor_post'].iloc[0]):\n",
    "        annotation.append(True)\n",
    "        continue\n",
    "    \n",
    "    # Segment the time series\n",
    "    thor_segments = segment_time_series(df_smooth['distance.origin-thor_post'].iloc[0])\n",
    "    prob_segments = segment_time_series(df_smooth['distance.origin-prob'].iloc[0])\n",
    "    \n",
    "    # Check each segment for correlation and significant change\n",
    "    for thor_seg, prob_seg in zip(thor_segments, prob_segments):\n",
    "        corr = calculate_correlation(thor_seg, prob_seg)\n",
    "        if corr < 0.8:\n",
    "            if significant_change(thor_seg) or significant_change(prob_seg):\n",
    "                annotation.append(True)\n",
    "            else:\n",
    "                annotation.append(False)\n",
    "        else:\n",
    "            annotation.append(False)\n",
    "            \n",
    "    annotations[exp_key] = annotation\n",
    "\n",
    "# Check first few annotations to see if the code works as expected\n",
    "first_few_annotations = {key: annotations[key] for key in list(annotations.keys())[:5]}\n",
    "first_few_annotations\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:58:44.234558Z",
     "start_time": "2023-09-13T09:58:44.019528Z"
    }
   },
   "id": "3863cfd68a3207f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trace = df['distance.origin-thor_post'][76]\n",
    "ts_thor_data_smooth = smooth_time_series(trace)\n",
    "plt.plot(trace)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T13:16:03.269305Z",
     "end_time": "2023-09-21T13:16:03.394355Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and annotation is your boolean list\n",
    "df['Annotation'] = annotation\n",
    "\n",
    "# Filter the DataFrame based on the Annotation column\n",
    "true_df = df[df['Annotation'] == True]\n",
    "false_df = df[df['Annotation'] == False]\n",
    "\n",
    "# Plot 'distance.origin-thor_post' for rows where Annotation is True\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for index, row in true_df.iterrows():\n",
    "    plt.plot(row['distance.origin-thor_post'])\n",
    "plt.title(\"Rows where Annotation is True\")\n",
    "\n",
    "# Plot 'distance.origin-thor_post' for rows where Annotation is False\n",
    "plt.subplot(1, 2, 2)\n",
    "for index, row in false_df.iterrows():\n",
    "    plt.plot(row['distance.origin-thor_post'])\n",
    "plt.title(\"Rows where Annotation is False\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T06:53:32.297766Z",
     "end_time": "2023-09-21T06:53:32.577293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and annotation is your boolean list\n",
    "df['Annotation'] = annotation\n",
    "\n",
    "# Filter the DataFrame based on the Annotation column\n",
    "false_df = df[df['Annotation'] == False]\n",
    "\n",
    "# Create a figure for each row where Annotation is False\n",
    "for index, row in false_df.iterrows():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot 'distance.origin-prob' and 'distance.origin-thor_post' in the same subplot\n",
    "    plt.plot(row['distance.origin-prob'], label='distance.origin-prob')\n",
    "    plt.plot(row['distance.origin-thor_post'], label='distance.origin-thor_post')\n",
    "\n",
    "    plt.title(f\"Row {index}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T07:59:15.737730Z",
     "end_time": "2023-09-21T07:59:16.050339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#one approach to achieve this to filter in a stepwise fashion. If there very little movement in the entire thorax trace then it is kept. Below code, then tries to identify the largest possible window where standard deviation remains below a threshold. If it does not find, then the trace is labeled as False. True labeled traces have Start and End indices that correspond to sections\n",
    "# Importing necessary libraries and modules\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for smoothing the time series using Savitzky-Golay filter\n",
    "def smooth_time_series(time_series, window_length=61, polyorder=3):\n",
    "    return savgol_filter(time_series, window_length, polyorder)\n",
    "\n",
    "# Function to check if the time series is stationary\n",
    "def is_stationary(time_series, threshold=10):\n",
    "    return np.max(time_series) - np.min(time_series) <= threshold\n",
    "\n",
    "# Function to segment the time series\n",
    "def segment_time_series(time_series, window_size=75):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start + window_size <= len(time_series):\n",
    "        segments.append(time_series[start:start + window_size])\n",
    "        start += window_size // 2  # 50% overlap\n",
    "    return segments\n",
    "\n",
    "# Function to calculate correlation between two segments\n",
    "def calculate_correlation(segment1, segment2):\n",
    "    return np.corrcoef(segment1, segment2)[0, 1]\n",
    "\n",
    "# Function to check for significant change in the segment\n",
    "def significant_change(segment, rate_threshold=15, points_threshold=30):\n",
    "    return np.max(segment) - np.min(segment) >= rate_threshold and len(segment) >= points_threshold\n",
    "\n",
    "def find_stable_window(time_series, window_size=60, threshold=2):\n",
    "    \"\"\"\n",
    "    Find the stable window in a time series according to specified conditions.\n",
    "\n",
    "    Parameters:\n",
    "        time_series (array-like): The time series data.\n",
    "        window_size (int): The size of the rolling window for calculating standard deviation.\n",
    "        threshold (float): The threshold for standard deviation to identify change.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The indices (start, end) of the stable window.\n",
    "    \"\"\"\n",
    "    # Calculate the midpoint of the time series\n",
    "    midpoint = len(time_series) // 2\n",
    "\n",
    "    # Calculate the rolling standard deviation\n",
    "    rolling_std = pd.Series(time_series).rolling(window=window_size, center=True).std()\n",
    "\n",
    "    # Step 2: From beginning to midpoint, find the last index where rolling_std exceeds the threshold\n",
    "    last_exceed_before_mid = rolling_std[:midpoint][rolling_std[:midpoint] > threshold].last_valid_index()\n",
    "    start = 0 if last_exceed_before_mid is None else last_exceed_before_mid + 1\n",
    "\n",
    "    # Step 3: From midpoint to end, find the first index where rolling_std exceeds the threshold\n",
    "    first_exceed_after_mid = rolling_std[midpoint:][rolling_std[midpoint:] > threshold].first_valid_index()\n",
    "    end = len(time_series) if first_exceed_after_mid is None else first_exceed_after_mid - 1\n",
    "\n",
    "    start = start + window_size\n",
    "\n",
    "    return start, end\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T13:28:40.723014Z",
     "end_time": "2023-09-21T13:28:40.741206Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bouts_dict_filt = {}  # Initialize new dictionary to hold filtered bouts\n",
    "\n",
    "# Main loop iterating through bouts_dict\n",
    "for exp_key, df in bouts_dict.items():\n",
    "    if \"distance.origin-thor_post\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    modified_data_list = []  # Initialize a list to hold modified data\n",
    "    start_indices = []  # Initialize a list to hold start indices\n",
    "    end_indices = []  # Initialize a list to hold end indices\n",
    "    annotation = []  # Initialize a list to hold annotations\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        ts_thor_data = row['distance.origin-thor_post']\n",
    "        ts_thor_data_smooth = smooth_time_series(ts_thor_data)\n",
    "\n",
    "        if is_stationary(ts_thor_data_smooth):\n",
    "            annotation.append(True)\n",
    "            modified_data_list.append(ts_thor_data)  # Save the entire ts_thor_data\n",
    "            start_indices.append(0)  # Start index is 0 for the entire series\n",
    "            end_indices.append(len(ts_thor_data) - 1)  # End index for the entire series\n",
    "            continue\n",
    "\n",
    "        start, end = find_stable_window(ts_thor_data_smooth)\n",
    "\n",
    "        if end - start <= 0 or len(ts_thor_data_smooth[start:end]) < 150:\n",
    "            annotation.append(False)\n",
    "            modified_data_list.append(None)  # Save None if conditions are not met\n",
    "            start_indices.append(None)  # No start index\n",
    "            end_indices.append(None)  # No end index\n",
    "            continue\n",
    "\n",
    "        annotation.append(True)\n",
    "        modified_data_list.append(ts_thor_data[start:end])  # Save the modified ts_thor_data\n",
    "        start_indices.append(start)  # Save the start index\n",
    "        end_indices.append(end)  # Save the end index\n",
    "\n",
    "    df['Annotation'] = annotation\n",
    "    df['ModifiedData'] = modified_data_list  # Add the modified data to the DataFrame\n",
    "    df['StartIndex'] = start_indices  # Add the start indices to the DataFrame\n",
    "    df['EndIndex'] = end_indices  # Add the end indices to the DataFrame\n",
    "    bouts_dict_filt[exp_key] = df  # Save the modified DataFrame into bouts_dict_filt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T13:28:42.715521Z",
     "end_time": "2023-09-21T13:28:43.740712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict the peaks in these\n",
    "\n",
    "config = utils.create_config()\n",
    "\n",
    "\n",
    "true_peak_annotations_df = utils.read_pickle('true_annotations.pkl')\n",
    "fly_db = utils.create_fly_database(bouts_dict_filt, true_peak_annotations_df=true_peak_annotations_df)\n",
    "\n",
    "for fly in fly_db.fly_data:\n",
    "    info_df = utils.get_model_prediction\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T15:24:18.980633Z",
     "end_time": "2023-09-21T15:24:19.584668Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "expt_info_df_path = r'Z:\\mfk\\basty-projects\\expt_info_df.pkl'\n",
    "expt_info_df = pd.read_pickle(expt_info_df_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T08:41:32.605329Z",
     "end_time": "2023-09-22T08:41:32.664744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def slice_correct_annotation(bouts_dict_filt):\n",
    "    bouts_edited_dict = {}\n",
    "\n",
    "    for key, df in bouts_dict_filt.items():\n",
    "        modified_rows = []\n",
    "\n",
    "        # Iterate through rows\n",
    "        for index, row in df.iterrows():\n",
    "            modified_row = row.copy()\n",
    "            if row['Annotation']:  # Assuming Annotation is a boolean flag\n",
    "                start = int(row['start_index'])\n",
    "                new_start = start + int(row['StartIndex'])\n",
    "                new_stop = new_start + int(row['EndIndex'])\n",
    "\n",
    "                # Update start and stop index\n",
    "                modified_row['start_index'] = new_start\n",
    "                modified_row['stop_index'] = new_stop\n",
    "\n",
    "                # Slice numpy arrays in the relevant columns\n",
    "                for col in df.columns:\n",
    "                    if isinstance(row[col], np.ndarray):\n",
    "                        old_list = row[col]\n",
    "                        modified_row[col] = old_list[int(row['StartIndex']):int(row['EndIndex']) + 1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            modified_rows.append(modified_row)\n",
    "\n",
    "        # Create a new DataFrame with modified rows\n",
    "        bouts_edited_dict[key] = pd.DataFrame(modified_rows)\n",
    "\n",
    "    return bouts_edited_dict\n",
    "\n",
    "def generate_tick_data(FPS=30, sd=False):\n",
    "\n",
    "    if sd == False:\n",
    "        xticks = np.arange(\n",
    "            start=0, stop=FPS * 60 * 60 * 16 + 1, step=FPS * 60 * 60 * 2\n",
    "        )\n",
    "        ZT_ticks = xticks\n",
    "        ZT_ticklabels = [\n",
    "            \"ZT\" + str((tick + 10) % 24) for tick in range(0, len(xticks) * 2, 2)\n",
    "        ]\n",
    "    else:\n",
    "        xticks = np.arange(\n",
    "            start=0, stop=FPS * 60 * 60 * 6 + 1, step=FPS * 60 * 60 * 1\n",
    "        )\n",
    "        ZT_ticks = xticks\n",
    "        ZT_ticklabels = [\n",
    "            \"ZT\" + str(tick) for tick in range(0, len(xticks) * 1, 1)\n",
    "        ]\n",
    "\n",
    "    return ZT_ticks, ZT_ticklabels\n",
    "\n",
    "def load_annotated_flies(expt_info_df):\n",
    "    basty_path = r'Z:\\mfk\\basty-projects'\n",
    "    annotated_dict = {}\n",
    "\n",
    "    for expt in expt_info_df.ExptNames:\n",
    "        folder_path = os.path.join(basty_path, expt, 'annotations')\n",
    "\n",
    "        if os.path.exists(folder_path):\n",
    "            annotation_paths = glob.glob(os.path.join(folder_path, '*Pumping.csv'))\n",
    "            if annotation_paths:  # Checking if list is not empty\n",
    "                annotation_path = annotation_paths[0]  # Taking the first match\n",
    "                annotated_dict[expt] = pd.read_csv(annotation_path)\n",
    "\n",
    "    return annotated_dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prob_pump_ethograms(bouts_dict_filt, annotated_dict, expt_info_df,SD):\n",
    "    idx_sd = expt_info_df[expt_info_df['SD'] == SD]\n",
    "    fig, ax = plt.subplots()\n",
    "    color = '#377eb8'  # The color to use for the bar\n",
    "    red_color = '#d62728'\n",
    "\n",
    "    i = 0  # Counter for which horizontal line (y-value) to plot on\n",
    "    valid_exptNames = []  # List to store valid experiment names\n",
    "\n",
    "    ZT_ticks, ZT_ticklabels = generate_tick_data(30, sd=SD)\n",
    "\n",
    "    for expts in idx_sd.ExptNames:\n",
    "        df = bouts_dict_filt.get(expts, None)\n",
    "\n",
    "        if df is None:  # Check if the experiment name exists in bouts_dict_filt\n",
    "            df = annotated_dict.get(expts, None)\n",
    "            if df is not None:\n",
    "\n",
    "                for idx, row in df.iterrows():\n",
    "                    start, stop = row['Start'], row['Finish']\n",
    "                    if expts in [\"Fly11242021_F_B_SD_5D_9am\", \"Fly11192021_F_A_SD_5D_9am\", \"Fly11152020_F_SD\",\"Fly11052021_F_A_SD_5D_8am\"]:\n",
    "                        if start < 1512000 or stop < 1512000:\n",
    "                            continue\n",
    "                        start, stop = start - 1512000, stop - 1512000\n",
    "                    ax.barh(i, stop - start, left=start, color=red_color, edgecolor='none', rasterized=True)\n",
    "                valid_exptNames.append(expts)\n",
    "                i += 1\n",
    "            continue\n",
    "\n",
    "        if not df['Annotation'].any():  # Skip if no True annotations\n",
    "            continue\n",
    "\n",
    "        valid_exptNames.append(expts)\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['Annotation']:  # Only plot if annotation is True\n",
    "                start, stop = row['start_index'], row['stop_index']\n",
    "                stop = start + row['EndIndex']\n",
    "                start = row['StartIndex'] + start\n",
    "                ax.barh(i, stop - start, left=start, color=color, edgecolor='none', rasterized=True)\n",
    "\n",
    "        i += 1  # Move to the next line for the next experiment\n",
    "\n",
    "    # Customizing the plot (you can add more here)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Experiment Name')\n",
    "    ax.set_yticks(range(len(valid_exptNames)))\n",
    "    ax.set_xticks(ZT_ticks)\n",
    "    ax.set_xticklabels(ZT_ticklabels)\n",
    "    ax.set_yticklabels(valid_exptNames)\n",
    "\n",
    "\n",
    "    plt.savefig(f'Fig_1 {SD}.pdf')\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T16:33:21.749449Z",
     "end_time": "2023-09-25T16:33:21.760474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "outputs": [],
   "source": [
    "bouts_edited_dict = slice_correct_annotation(bouts_dict_filt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T16:33:24.074616Z",
     "end_time": "2023-09-25T16:33:24.633264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [
    {
     "data": {
      "text/plain": "   start_index  stop_index               region  \\\n0        26510       28105  Fly08022022_6d_SD_B   \n1       130126      131384  Fly08022022_6d_SD_B   \n2       164295      165605  Fly08022022_6d_SD_B   \n3       439906      441017  Fly08022022_6d_SD_B   \n\n                                         pose.prob_x  \\\n0  [560.2674867659807, 563.4234387129545, 566.579...   \n1  [895.3112794607878, 895.1558222522339, 895.000...   \n2  [932.1198892593384, 931.9556339581808, 931.879...   \n3  [1001.2057900031408, 1001.5866277217865, 1001....   \n\n                                         pose.prob_y  \\\n0  [376.944912130634, 374.4901364892721, 371.6412...   \n1  [150.66675655047098, 150.65931006272635, 150.6...   \n2  [621.9242204229037, 622.0024285068115, 621.823...   \n3  [470.04109370708466, 470.20261603593826, 470.3...   \n\n                                         pose.halt_x  \\\n0  [740.2003645102183, 739.9911094071964, 739.681...   \n1  [897.2066330115, 897.2326778968176, 897.258722...   \n2  [752.4093451313674, 752.6971930861473, 752.695...   \n3  [862.2910725275675, 862.2778785228729, 862.275...   \n\n                                         pose.halt_y  \\\n0  [361.2953839302063, 361.13586648305255, 360.97...   \n1  [339.5045042037964, 339.46439003944397, 339.42...   \n2  [595.462761554867, 595.9712180693945, 596.3370...   \n3  [355.19462289412814, 355.0527958869934, 355.14...   \n\n                                    pose.thor_post_x  \\\n0  [759.0354959170023, 759.0294677217802, 758.802...   \n1  [862.1325496435165, 861.999044418335, 861.8655...   \n2  [757.0845412413279, 757.225207666556, 757.2043...   \n3  [831.1597726643085, 830.9471826752027, 831.026...   \n\n                                    pose.thor_post_y  \\\n0  [401.19881904125214, 401.02297647794086, 400.8...   \n1  [342.9556700189908, 342.9983994960785, 342.945...   \n2  [562.5580627123514, 562.6832326352596, 562.726...   \n3  [389.5113400419553, 389.5192061463992, 389.562...   \n\n                                distance.origin-halt  ...  \\\n0  [823.6691897056337, 823.4111707173753, 823.063...  ...   \n1  [959.2929952285162, 959.3031587515733, 959.313...  ...   \n2  [959.5289068285341, 960.0701835006035, 960.296...  ...   \n3  [932.581960952304, 932.5157519619052, 932.5484...  ...   \n\n                           distance.origin-thor_post  \\\n0  [858.5425886128569, 858.4551010584281, 858.172...   \n1  [927.8421874182083, 927.7339352610716, 927.590...   \n2  [943.2118407384769, 943.3994039718938, 943.408...   \n3  [917.9028552720952, 917.7136984663451, 917.804...   \n\n                                  distance.head-prob  \\\n0  [66.63783410754098, 65.34948492424904, 65.0285...   \n1  [87.89273998615339, 87.80470302158548, 87.5877...   \n2  [88.52484354528991, 88.18473874507596, 87.8602...   \n3  [82.98369185772786, 83.13882496129297, 83.3214...   \n\n                             distance.thor_post-halt  \\\n0  [44.12534769010063, 44.197744637693894, 44.218...   \n1  [35.24346562964858, 35.41042432583423, 35.5678...   \n2  [33.23517210100221, 33.594536613913384, 33.911...   \n3  [46.33351818047191, 46.57838488471763, 46.4889...   \n\n  distance.avg(thor_post-joint1,thor_post-joint2,thor_post-joint3)  \\\n0  [181.1475453188581, 180.98023535151233, 180.88...                 \n1  [160.47512030666215, 160.5446340061484, 160.56...                 \n2  [172.65856018119766, 172.92136918763921, 173.0...                 \n3  [177.97973925502473, 178.258162379317, 178.174...                 \n\n  distance.avg(origin-joint1,origin-joint2,origin-joint3)  \\\n0  [748.3288001704823, 748.2642331183318, 748.115...        \n1  [1008.4972109530501, 1008.5044679774234, 1008....        \n2  [1057.1972576159126, 1057.702175561525, 1057.9...        \n3  [1028.804927253763, 1028.8074517790158, 1028.8...        \n\n                      distance.origin-prob_segmented Annotation  \\\n0  [[675.2681863587532, 676.5270383501934, 677.59...       True   \n1  [[907.9001920140515, 907.7456548068129, 907.59...      False   \n2  [[1120.5521959736984, 1120.4589794966907, 1120...       True   \n3  [[1106.0522879634423, 1106.4656673201118, 1106...       True   \n\n                                        ModifiedData StartIndex  EndIndex  \n0  [865.07023617753, 864.7159967523028, 864.50039...      104.0    1595.0  \n1                                               None        NaN       NaN  \n2  [943.2118407384769, 943.3994039718938, 943.408...        0.0    1310.0  \n3  [917.9028552720952, 917.7136984663451, 917.804...        0.0    1111.0  \n\n[4 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start_index</th>\n      <th>stop_index</th>\n      <th>region</th>\n      <th>pose.prob_x</th>\n      <th>pose.prob_y</th>\n      <th>pose.halt_x</th>\n      <th>pose.halt_y</th>\n      <th>pose.thor_post_x</th>\n      <th>pose.thor_post_y</th>\n      <th>distance.origin-halt</th>\n      <th>...</th>\n      <th>distance.origin-thor_post</th>\n      <th>distance.head-prob</th>\n      <th>distance.thor_post-halt</th>\n      <th>distance.avg(thor_post-joint1,thor_post-joint2,thor_post-joint3)</th>\n      <th>distance.avg(origin-joint1,origin-joint2,origin-joint3)</th>\n      <th>distance.origin-prob_segmented</th>\n      <th>Annotation</th>\n      <th>ModifiedData</th>\n      <th>StartIndex</th>\n      <th>EndIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26510</td>\n      <td>28105</td>\n      <td>Fly08022022_6d_SD_B</td>\n      <td>[560.2674867659807, 563.4234387129545, 566.579...</td>\n      <td>[376.944912130634, 374.4901364892721, 371.6412...</td>\n      <td>[740.2003645102183, 739.9911094071964, 739.681...</td>\n      <td>[361.2953839302063, 361.13586648305255, 360.97...</td>\n      <td>[759.0354959170023, 759.0294677217802, 758.802...</td>\n      <td>[401.19881904125214, 401.02297647794086, 400.8...</td>\n      <td>[823.6691897056337, 823.4111707173753, 823.063...</td>\n      <td>...</td>\n      <td>[858.5425886128569, 858.4551010584281, 858.172...</td>\n      <td>[66.63783410754098, 65.34948492424904, 65.0285...</td>\n      <td>[44.12534769010063, 44.197744637693894, 44.218...</td>\n      <td>[181.1475453188581, 180.98023535151233, 180.88...</td>\n      <td>[748.3288001704823, 748.2642331183318, 748.115...</td>\n      <td>[[675.2681863587532, 676.5270383501934, 677.59...</td>\n      <td>True</td>\n      <td>[865.07023617753, 864.7159967523028, 864.50039...</td>\n      <td>104.0</td>\n      <td>1595.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>130126</td>\n      <td>131384</td>\n      <td>Fly08022022_6d_SD_B</td>\n      <td>[895.3112794607878, 895.1558222522339, 895.000...</td>\n      <td>[150.66675655047098, 150.65931006272635, 150.6...</td>\n      <td>[897.2066330115, 897.2326778968176, 897.258722...</td>\n      <td>[339.5045042037964, 339.46439003944397, 339.42...</td>\n      <td>[862.1325496435165, 861.999044418335, 861.8655...</td>\n      <td>[342.9556700189908, 342.9983994960785, 342.945...</td>\n      <td>[959.2929952285162, 959.3031587515733, 959.313...</td>\n      <td>...</td>\n      <td>[927.8421874182083, 927.7339352610716, 927.590...</td>\n      <td>[87.89273998615339, 87.80470302158548, 87.5877...</td>\n      <td>[35.24346562964858, 35.41042432583423, 35.5678...</td>\n      <td>[160.47512030666215, 160.5446340061484, 160.56...</td>\n      <td>[1008.4972109530501, 1008.5044679774234, 1008....</td>\n      <td>[[907.9001920140515, 907.7456548068129, 907.59...</td>\n      <td>False</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>164295</td>\n      <td>165605</td>\n      <td>Fly08022022_6d_SD_B</td>\n      <td>[932.1198892593384, 931.9556339581808, 931.879...</td>\n      <td>[621.9242204229037, 622.0024285068115, 621.823...</td>\n      <td>[752.4093451313674, 752.6971930861473, 752.695...</td>\n      <td>[595.462761554867, 595.9712180693945, 596.3370...</td>\n      <td>[757.0845412413279, 757.225207666556, 757.2043...</td>\n      <td>[562.5580627123514, 562.6832326352596, 562.726...</td>\n      <td>[959.5289068285341, 960.0701835006035, 960.296...</td>\n      <td>...</td>\n      <td>[943.2118407384769, 943.3994039718938, 943.408...</td>\n      <td>[88.52484354528991, 88.18473874507596, 87.8602...</td>\n      <td>[33.23517210100221, 33.594536613913384, 33.911...</td>\n      <td>[172.65856018119766, 172.92136918763921, 173.0...</td>\n      <td>[1057.1972576159126, 1057.702175561525, 1057.9...</td>\n      <td>[[1120.5521959736984, 1120.4589794966907, 1120...</td>\n      <td>True</td>\n      <td>[943.2118407384769, 943.3994039718938, 943.408...</td>\n      <td>0.0</td>\n      <td>1310.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>439906</td>\n      <td>441017</td>\n      <td>Fly08022022_6d_SD_B</td>\n      <td>[1001.2057900031408, 1001.5866277217865, 1001....</td>\n      <td>[470.04109370708466, 470.20261603593826, 470.3...</td>\n      <td>[862.2910725275675, 862.2778785228729, 862.275...</td>\n      <td>[355.19462289412814, 355.0527958869934, 355.14...</td>\n      <td>[831.1597726643085, 830.9471826752027, 831.026...</td>\n      <td>[389.5113400419553, 389.5192061463992, 389.562...</td>\n      <td>[932.581960952304, 932.5157519619052, 932.5484...</td>\n      <td>...</td>\n      <td>[917.9028552720952, 917.7136984663451, 917.804...</td>\n      <td>[82.98369185772786, 83.13882496129297, 83.3214...</td>\n      <td>[46.33351818047191, 46.57838488471763, 46.4889...</td>\n      <td>[177.97973925502473, 178.258162379317, 178.174...</td>\n      <td>[1028.804927253763, 1028.8074517790158, 1028.8...</td>\n      <td>[[1106.0522879634423, 1106.4656673201118, 1106...</td>\n      <td>True</td>\n      <td>[917.9028552720952, 917.7136984663451, 917.804...</td>\n      <td>0.0</td>\n      <td>1111.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows Ã— 21 columns</p>\n</div>"
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bouts_dict_filt['Fly08022022_6d_SD_B']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T16:33:24.643233Z",
     "end_time": "2023-09-25T16:33:24.732237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1106.05228796, 1106.46566732, 1106.85623196, ..., 1107.77310847,\n       1107.65217075, 1107.57849717])"
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bouts_edited_dict['Fly08022022_6d_SD_B']['distance.origin-prob'][3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T09:39:28.315694Z",
     "end_time": "2023-09-26T09:39:28.322204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotated_dict = load_annotated_flies(expt_info_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T16:10:56.941437Z",
     "end_time": "2023-09-22T16:10:57.128935Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prob_pump_ethograms(bouts_dict_filt,annotated_dict,expt_info_df,SD=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T16:10:59.180907Z",
     "end_time": "2023-09-22T16:11:00.528897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prob_pump_ethograms(bouts_dict_filt,annotated_dict,expt_info_df,SD=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T10:27:02.363003Z",
     "end_time": "2023-09-22T10:27:02.397788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Predictions'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fruitflyvideo-aDSQpkQ7-py3.9\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fruitflyvideo-aDSQpkQ7-py3.9\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fruitflyvideo-aDSQpkQ7-py3.9\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Predictions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[496], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m anomalies \u001B[38;5;241m=\u001B[39m info_df\u001B[38;5;241m.\u001B[39mloc[info_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistance.origin-prob\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m      8\u001B[0m group_pred_idx, group_pred_val \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mfilter_prediction(anomalies, grouped_range\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgrouped_range\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 10\u001B[0m \u001B[43mbouts_edited_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfly\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPredictions\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;28mint\u001B[39m(fly\u001B[38;5;241m.\u001B[39mtrial_id)] \u001B[38;5;241m=\u001B[39m group_pred_idx\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fruitflyvideo-aDSQpkQ7-py3.9\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fruitflyvideo-aDSQpkQ7-py3.9\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Predictions'"
     ]
    }
   ],
   "source": [
    "true_peak_annotations_df = utils.read_pickle('true_annotations.pkl')\n",
    "fly_db = utils.create_fly_database(bouts_edited_dict, true_peak_annotations_df=true_peak_annotations_df)\n",
    "\n",
    "for fly in fly_db.fly_data:\n",
    "    info_df = utils.get_model_prediction(fly, config, bouts_edited_dict)\n",
    "\n",
    "    anomalies = info_df.loc[info_df['predictions'] == -1, ['distance.origin-prob']]\n",
    "    group_pred_idx, group_pred_val = utils.filter_prediction(anomalies, grouped_range=config['grouped_range'])\n",
    "\n",
    "    fly.peak_index = group_pred_idx\n",
    "    fly.peak_values = group_pred_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T10:37:56.197841Z",
     "end_time": "2023-09-25T10:44:13.989346Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "fly_db.fly_data[0].peak_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T09:54:15.786638Z",
     "end_time": "2023-09-26T09:54:15.805147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [
    {
     "data": {
      "text/plain": "[278, 405, 509, 920]"
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_pred_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-26T09:40:34.734665Z",
     "end_time": "2023-09-26T09:40:34.754665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "data": {
      "text/plain": "[278, 405, 509, 920]"
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_pred_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-25T16:34:32.443769Z",
     "end_time": "2023-09-25T16:34:32.553770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
